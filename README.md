# Task 3: Image Captioning

**Author:** Sunny Kumar, B.Tech CSE 2nd Year  

---

## ğŸ“Œ Description  
This project implements **Image Captioning** using Deep Learning.  
It combines **Convolutional Neural Networks (CNNs)** for image feature extraction and **Recurrent Neural Networks (RNNs)** for sequence modeling to generate descriptive captions for images.  

---

## âš™ï¸ Steps to Run  
1. Open the Google Colab notebook.  
2. Install dependencies (`TensorFlow`, `Keras`, etc.).  
3. Load dataset (Flickr8k, Flickr30k, or MS COCO).  
4. Extract features using a pre-trained CNN (InceptionV3 / ResNet).  
5. Train the Encoder-Decoder model with LSTM.  
6. Generate captions for unseen images.  

---

## ğŸ–¼ï¸ Sample Output  
<img width="726" height="591" alt="Screenshot 2025-08-22 225422" src="https://github.com/user-attachments/assets/e475aadb-c0e9-4b79-9ead-306dadd08c12" />

**Generated Caption:** *"A dog is standing on the grass and looking to the side."*  

---

## ğŸ“š Conclusion  
Image Captioning showcases how **Computer Vision** and **Natural Language Processing (NLP)** can be combined.  
It has applications in **accessibility**, **autonomous systems**, and **human-computer interaction**.  
